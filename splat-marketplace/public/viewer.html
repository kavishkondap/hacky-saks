<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>WebXR Splat Viewer + Capture</title>
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <!-- Self-hosted scripts so COEP/COOP don't block them -->
    <script src="/vendor/aframe.min.js"></script>
    <script src="/vendor/aframe-gaussian-splatting.js"></script>

    <style>
      html, body { margin:0; height:100%; background:#000; }
      .hud {
        position: fixed; left: 8px; bottom: 8px; right: 8px; z-index: 20;
        background: rgba(0,0,0,.55); color: #fff; padding: 8px 10px; border-radius: 10px;
        font: 12px/1.45 ui-monospace, SFMono-Regular, Menlo, monospace;
      }
      .row { display:flex; gap:8px; flex-wrap: wrap; align-items: center; margin: 4px 0; }
      .mono { white-space: pre-wrap; }
      .ok { color:#9fe; } .warn { color:#ff9; } .err { color:#f99; }
      button { cursor:pointer; }
    </style>
  </head>
  <body>
    <div class="hud">
      <div class="row">Hold <b>A</b> on the right controller to record. Release to stop.</div>
      <div id="status" class="row mono ok">idle</div>
      <div class="row"><button id="desktopTest">Desktop test (hold mouse)</button></div>
      <div id="result" class="row mono"></div>
    </div>

    <a-scene renderer="antialias: false" stats>
      <!-- Lights + sky so you always see *something* -->
      <a-entity light="type: ambient; color: #BBB; intensity: 0.8"></a-entity>
      <a-entity light="type: directional; color: #FFF; intensity: 0.9" position="1 3 -2"></a-entity>
      <a-sky color="#111"></a-sky>

      <!-- Camera rig -->
      <a-entity id="rig" position="0 1.6 0">
        <a-entity id="camera" camera look-controls wasd-controls></a-entity>
      </a-entity>

      <!-- Visual sanity check (use flat shader so lights aren’t required) -->
      <a-entity position="0 1.6 -2"
                animation="property: rotation; to: 0 360 0; dur: 10000; easing: linear; loop: true">
        <a-sphere position="0 0 0.5" radius="0.5" color="#FFFFFF" material="shader: flat"></a-sphere>
        <a-sphere position="0 0 -0.5" radius="0.5" color="#EF2D5E" material="shader: flat"></a-sphere>
      </a-entity>

      <!-- Controllers -->
      <a-entity id="left-controller"  oculus-touch-controls="hand: left"></a-entity>
      <a-entity id="right-controller" oculus-touch-controls="hand: right"
                capture-on-hold="button: a"></a-entity>
      
      <!-- NEW: speaker used to play the synthesized reply -->
      <a-entity id="speaker" sound="positional: false; volume: 1.0"></a-entity>

      <!-- Gaussian splat (underscore attribute!) -->
      <a-entity id="splat-viewer"
                position="0 1.5 -2" rotation="0 0 0" scale="1 1 1"
                gaussian_splatting="src: /public/train.splat;">
      </a-entity>
    </a-scene>

    <script>
      // If ?src= is provided, use it as-is (absolute or relative URL)
      (function () {
        const params = new URLSearchParams(window.location.search);
        const el = document.getElementById('splat-viewer');
        if (params.has('src')) {
          const sourcePath = params.get('src');           // e.g. "public/train.splat"
          el.setAttribute('gaussian_splatting', `src: https://calhacks-hacky-saks.s3.us-east-2.amazonaws.com/${sourcePath};`);
        }
        if (params.has('rot')) {
          const rotation = params.get('rot');
          el.setAttribute('rotation', rotation);
        }
        if (params.has('pos')) {
          const position = params.get('pos');
          el.setAttribute('position', position);
        }
      })();

      const statusEl = document.getElementById('status');
      const resultEl = document.getElementById('result');
      function status(msg, cls='ok'){ statusEl.className = 'row mono '+cls; statusEl.textContent = msg; }

      function blobToBase64(blob) {
        return new Promise((resolve, reject) => {
          const reader = new FileReader();
          reader.onloadend = () => resolve(String(reader.result).split(',')[1]||'');
          reader.onerror = reject;
          reader.readAsDataURL(blob);
        });
      }

      // --- Capture logic (unchanged) ---
      AFRAME.registerComponent('capture-on-hold', {
        schema: { button: { type:'string', default:'a' } },
        init() {
          this.mediaStream = null;
          this.mediaRecorder = null;
          this.chunks = [];
          this.recording = false;
          this._pendingScreenshot = null;

          const btn = this.data.button.toLowerCase();
          this.onDown = () => this.startCapture();
          this.onUp   = () => this.stopCapture();
          this.el.addEventListener(`${btn}buttondown`, this.onDown);
          this.el.addEventListener(`${btn}buttonup`,   this.onUp);
        },
        async startCapture() {
          if (this.recording) return;
          this.recording = true;

          const scene = this.el.sceneEl;
          const canvas = scene.canvas || scene.renderer?.domElement;
          const screenshotBlob = await new Promise(res => canvas.toBlob(res, 'image/png'));
          this._pendingScreenshot = screenshotBlob;

          try {
            this.mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
          } catch (e) { status('mic permission denied', 'err'); this.recording = false; return; }
          this.chunks = [];
          this.mediaRecorder = new MediaRecorder(this.mediaStream, { mimeType: 'audio/webm' });
          this.mediaRecorder.ondataavailable = e => { if (e.data.size) this.chunks.push(e.data); };
          this.mediaRecorder.start();
          status('recording… (release A to stop)');
        },
        async stopCapture() {
          if (!this.recording) return;
          this.recording = false;

          await new Promise(resolve => { this.mediaRecorder.onstop = resolve; this.mediaRecorder.stop(); });
          this.mediaStream.getTracks().forEach(t => t.stop());
          const audioBlob = new Blob(this.chunks, { type: 'audio/webm' });
          status('transcribing (Fish)…');

          let transcript = '';
          try {
            const form = new FormData();
            form.append('audio', audioBlob, 'clip.webm');
            const r = await fetch('/api/asr', { method: 'POST', body: form });
            const j = await r.json();
            if (!r.ok) throw new Error(j.error || 'asr_failed');
            transcript = j.text || '';
          } catch (e) { status('Fish STT failed: ' + e.message, 'err'); }

          status('asking Claude…');
          let output = '';
          try {
            const img = await blobToBase64(this._pendingScreenshot);
            const r = await fetch('/api/claude', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                transcript,
                image_base64: img,
                prompt: 'Analyze the VR scene image and the user transcript. Return a concise, helpful answer.'
              })
            });
            const j = await r.json();
            if (!r.ok) throw new Error(j.error || 'claude_failed');
            output = j.output || '';
          } catch (e) { status('Claude failed: ' + e.message, 'err'); }

          window.lastClaudeOutput = output;
          resultEl.textContent = output || '(no output)';
          status('done.');
          this._pendingScreenshot = null;
          this.mediaStream = null; this.mediaRecorder = null; this.chunks = [];
        }
      });

      // Desktop test button
      (function() {
        const btn = document.getElementById('desktopTest');
        let rec=null, stream=null, chunks=[], screenshotBlob=null;
        btn.onmousedown = async () => {
          const scene = document.querySelector('a-scene');
          const canvas = scene.canvas || scene.renderer?.domElement;
          screenshotBlob = await new Promise(res => canvas.toBlob(res, 'image/png'));
          try { stream = await navigator.mediaDevices.getUserMedia({ audio: true }); }
          catch (e) { status('mic denied (desktop test)', 'err'); return; }
          chunks=[]; rec = new MediaRecorder(stream, { mimeType: 'audio/webm' });
          rec.ondataavailable = e => { if (e.data.size) chunks.push(e.data); };
          rec.start(); status('recording… (release mouse to stop)');
        };
        btn.onmouseup = async () => {
          if (!rec) return;
          await new Promise(r => { rec.onstop = r; rec.stop(); });
          stream.getTracks().forEach(t => t.stop());
          const audioBlob = new Blob(chunks, { type:'audio/webm' });

          let transcript=''; try{
            const f = new FormData(); f.append('audio', audioBlob, 'clip.webm');
            const r = await fetch('/api/asr', { method:'POST', body:f });
            const j = await r.json(); if(!r.ok) throw new Error(j.error||'asr_failed');
            transcript = j.text||'';
          }catch(e){ status('Fish STT failed: '+e.message,'err'); }

          try{
            const b64 = await blobToBase64(screenshotBlob);
            const r = await fetch('/api/claude', {
              method:'POST', headers:{'Content-Type':'application/json'},
              body: JSON.stringify({ transcript, image_base64: b64, prompt: 'Describe key details and insights.' })
            });
            const j = await r.json(); if(!r.ok) throw new Error(j.error||'claude_failed');
            window.lastClaudeOutput = j.output||''; resultEl.textContent = window.lastClaudeOutput; status('done.');
          }catch(e){ status('Claude failed: '+e.message,'err'); }
        };
        btn.onmouseleave = btn.onmouseup;
      })();

      // Helpful console diagnostics
      window.addEventListener('error', (e) => console.error('Viewer error:', e.error || e.message));
      AFRAME.scenes && AFRAME.scenes[0]?.addEventListener('loaded', () => console.log('A-Frame scene loaded'));

      // === Right-trigger capture: screenshot + mic; then Fish ASR -> Claude -> Fish TTS ===
        AFRAME.registerComponent('trigger-capture', {
          init() {
            this.recording = false;
            this.mediaStream = null;
            this.mediaRecorder = null;
            this.chunks = [];
            this.screenshotBlob = null;

            this.onDown = () => this.startCapture();
            this.onUp   = () => this.stopAndProcess();

            this.el.addEventListener('triggerdown', this.onDown);
            this.el.addEventListener('triggerup',   this.onUp);
          },

          async startCapture() {
            if (this.recording) return;
            this.recording = true;

            // 1) Screenshot immediately
            const scene = this.el.sceneEl;
            const canvas = scene.canvas || scene.renderer?.domElement;
            this.screenshotBlob = await new Promise(res => canvas.toBlob(res, 'image/png'));

            // 2) Start mic
            try {
              this.mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
            } catch (e) { status('mic permission denied', 'err'); this.recording = false; return; }

            this.chunks = [];
            this.mediaRecorder = new MediaRecorder(this.mediaStream, { mimeType: 'audio/webm' });
            this.mediaRecorder.ondataavailable = e => { if (e.data.size) this.chunks.push(e.data); };
            this.mediaRecorder.start();
            status('recording… (release trigger to stop)');
          },

          async stopAndProcess() {
            if (!this.recording) return;
            this.recording = false;

            // stop mic + collect audio
            await new Promise(r => { this.mediaRecorder.onstop = r; this.mediaRecorder.stop(); });
            this.mediaStream.getTracks().forEach(t => t.stop());
            const audioBlob = new Blob(this.chunks, { type: 'audio/webm' });

            // 3.1) Fish ASR
            status('transcribing (Fish)…');
            let transcript = '';
            try {
              const form = new FormData();
              form.append('audio', audioBlob, 'clip.webm');
              const r = await fetch('/api/asr', { method: 'POST', body: form });
              const j = await r.json();
              if (!r.ok) throw new Error(j.error || 'asr_failed');
              transcript = j.text || '';
            } catch (e) {
              status('Fish STT failed: ' + e.message, 'err');
            }

            // 3.2) Claude
            status('asking Claude…');
            let answerText = '';
            try {
              const imgB64 = await blobToBase64(this.screenshotBlob);
              const r = await fetch('/api/claude', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                  transcript,
                  image_base64: imgB64,
                  prompt: 'Analyze the VR scene image and the user transcript. Reply concisely and helpfully for audio playback.'
                })
              });
              const j = await r.json();
              if (!r.ok) throw new Error(j.error || 'claude_failed');
              answerText = (j.output || '').trim();
            } catch (e) {
              status('Claude failed: ' + e.message, 'err');
            }

            // HUD
            window.lastClaudeOutput = answerText;
            resultEl.textContent = answerText || '(no output)';

            // 3.3) Fish TTS → play in-headset
            if (answerText) {
              status('synthesizing (Fish TTS)…');
              try {
                const r = await fetch('/api/tts', {
                  method: 'POST',
                  headers: { 'Content-Type': 'application/json' },
                  body: JSON.stringify({ text: answerText, format: 'mp3' })
                });
                if (!r.ok) throw new Error(await r.text() || 'tts_failed');

                const buf = await r.arrayBuffer();
                const url = URL.createObjectURL(new Blob([buf], { type: 'audio/mpeg' }));

                // Play via A-Frame sound (works on Quest)
                const speaker = document.querySelector('#speaker');
                speaker.setAttribute('sound', 'src', url);
                if (speaker.components.sound) {
                  try { speaker.components.sound.stop(); } catch {}
                  speaker.components.sound.playSound();
                } else {
                  new Audio(url).play().catch(()=>{});
                }
                status('done.');
              } catch (e) {
                status('TTS failed: ' + e.message, 'err');
              }
            } else {
              status('done.');
            }

            // cleanup
            this.mediaStream = null;
            this.mediaRecorder = null;
            this.chunks = [];
            this.screenshotBlob = null;
          }
        });


    </script>
  </body>
</html>
